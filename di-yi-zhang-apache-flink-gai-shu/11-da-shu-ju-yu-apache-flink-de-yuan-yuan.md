# 追本溯源

在全面了解Apache Flink(后续简称 Flink)核心功能之前，我们内心往往会有一些疑问需要被解答，比如：Flink是在什么背景下诞生的？我为什么要学习Flink？Flink的发展历程是怎样的？总之我们要对任何技术和知识保持追其根本，探其源头的精神，最终达到知其然，知其所以然！

# 大数据\(Big Data\)

## 数据量与日俱增

随着云计算、物联网、人工智能等信息技术的快速发展，数据量呈现几何级增长，我们先看一份预测数据，全球数据总量在短暂的10年时间会由16.1ZB增长到163ZB，数据量的快速增长已经远远超越单个计算机存储和处理能力，如下：

![](/assets/import.png)

上图我们数据量的单位是ZB，我们简单介绍一下数据量的统计单位，基本单位是bit，按顺序给出所有单位：bit、Byte、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB。他们之间的转换关系是：

* 1 Byte =8 bit
* 1 KB = 1,024 Bytes 
* 1 MB = 1,024 KB 
* 1 GB = 1,024 MB 
* 1 TB = 1,024 GB 
* 1 PB = 1,024 TB 
* 1 EB = 1,024 PB 
* 1 ZB = 1,024 EB 
* 1 YB = 1,024 ZB 
* 1 BB = 1,024 YB 
* 1 NB = 1,024 BB 
* 1 DB = 1,024 NB 

看到上面的数据量也许我们会质疑全球数据真的有这么恐怖吗？数据都从哪里来的呢? 其实我看到这个数据也深表质疑，但是仔细查阅了一下资料，发现全球数据的确在快速的增长着，比如 Fecebook社交平台每天有几百亿，上千亿的照片数据，纽约证券交易每天有几TB的交易数据，再说说刚刚发生的阿里巴巴2018年双11数据，从交易额上创造了2135亿的奇迹，从数据量上看仅仅是Alibaba内部的监控日志处理看就达到了162GB/秒。所以Alibaba为代表的互联网行业，也促使了数据量的急速增长，同样以Alibaba双11近10年来的成交额来用数字证明数据的增长，如下：

![](/assets/alibaba11.png)

## 大数据分析

我们如何让大数据产生价值呢？毋庸置疑，对大数据进行统计分析，让那个统计分析的结果帮助我们进行决策。比如 推荐系统，我们可以根据一个用户长期的购买习惯，购买记录来分析其兴趣爱好，比如XXX（TODO）， 进而可以准确的进行有效推荐。那么面对上面的海量数据，在一台计算机上无法处理，那么我们如何在有限的时间内对全部数据进行统计分析呢？提及这个问题，我们不得不感谢Google发布的三大论文：

* GFS - 2003年，Google发布Google File System论文，这是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。
* MapReduce - 2004年， Google发布了MapReduce论文，论文描述了大数据的分布式计算方式，主要思想是将任务分解然后在多台处理能力较弱的计算节点中同时处理，然后将结果合并从而完成大数据处理。Mapreduce是针对分布式并行计算的一套编程模型，如下图所示：

![](/assets/MapReduce2.png)

* BigTable - 2006年, Google由发布了BigTable论文，是一款典型是NoSQL分布式数据库。

受益于Google的三大论文，Apache开源社区迅速开发了Hadoop生态系统，HDFS，MapReduce编程模型，NoSQL数据库HBase。并很快得到了全球学术界和工业界的普遍关注，并得到推广和普及应用。其中Alibaba在2008年就启动了基于hadoop的云梯项目，hadoop就成为了Alibaba分布式计算的核心技术体系，并在2010年就达到了千台机器的集群，hadoop在Alibaba的集群发展如下：

![](/assets/yunti.png)


但利用Hadoop进行MapReduce的开发，需要开发人员精通Java语言，并要对apReduce的运行原理有一定的了解，这样在一定程度上提高了MapReduce的开发门槛，所以在开源社区又不断涌现了一些为了简化MapReduce开发的开源框架，其中Hive就是典型的代表。HSQL可以让用户以类SQL的方式描述MapReduce计算，比如原本需要几十行，甚至上百行才能完成的wordCount，用户一条SQL语句就能完成了，这样极大的降低了MapReduce的开发门槛。这样Hadoop技术生态不断发展，基于Hadoop的分布式的大数据计算逐渐普及，在业界家喻户晓！

## 数据时效性

每一条数据都是一条信息，信息的时效性是指从信息源发送信息后经过接收、加工、传递、利用的时间间隔及其效率。时间间隔越短，时效性越强。一般时效性越强，信息所带来的价值越大，比如一个偏好推荐场景，用户在购买了一个“蒸箱”，如果能在秒级时间间隔给用户推荐一个“烤箱”的优惠产品，那么用户购买“烤箱”的概率会很高，那么在1天之后根据用户购买“蒸箱”的数据，分析出用户可能需要购买“烤箱”，那么我想这条推荐信息被用户采纳的可能性将大大降低。基于这样数据时效性问题，也暴露了Hadoop批量计算的弊端，就是实时性不高。基于这样的时代需求，典型的实时计算平台也应时而生，2009年Spark诞生于UCBerkeley的AMP实验室， 2010年Storm的核心概念于BackType被Nathan提出。Flink也以一个研究性的项目与2010年开始于德国柏林。

# Flink 的发展历史

Flink因分布式实时数据计算而产生，Flink以其优秀的纯流式计算架构很快于2014年3月成为Apache孵化器项目，2014年12月，Flink被接受为Apache顶级项目。随后社区非常活跃，Flink版本不断更新发布，截止到此刻(2018.11.25)社区已经发起了Flink-1.7.0的第三次后续发布版本的投票。如下鱼骨图介绍Flink的发展和发布历程：


